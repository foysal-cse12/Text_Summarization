{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "exact-incentive",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import hashlib\n",
    "import struct\n",
    "import subprocess\n",
    "import collections\n",
    "import tensorflow as tf\n",
    "from tensorflow.core.example import example_pb2\n",
    "import nltk\n",
    "import pandas as pd\n",
    "\n",
    "import re\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "irish-funeral",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('G:\\\\master thesis\\\\bigPatent_dataset_csv\\\\train\\\\val_pgn_E.csv')\n",
    "##data['content'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "coordinated-decline",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the invention comprises a pair of tubes with a first tube having a top and bottom . the bottom of the first tube comprises an open bottom and has an interior profile , the preferred profile is frusto - conical . the second tube having a top and bottom . the bottom of the second tube having a preferred exterior profile that is adapted to mate with the interior profile of the first tube , thereby sealing the open bottom of the first tube when the profiles come into contact with each other . open slots are provided for in the profile of the second tube to allow transfer of material from the interior of the second tube out through the open bottom of the first tube when the profiles disengage . a more specific detail of the preferred embodiment is further discussed below . the preferred embodiment 100 comprises an exterior casing 6 and an interior mandrel 5 . see , e . g , fig1 . the casing 6 and mandrel 5 may be of any cross section so long as mandrel 5 is able to fit slidably within casing 6 . each of the tubes have an upper end and a lower end . at the upper end of mandrel 5 is a first pile driver receiving plate 1 , which receives direct blows from a pile driver . pile driver plate 1 in turn rest on a second plate 2 having an annular opening , whereby mandrel 5 may be inserted there through . plate 2 is attached to the top end of casing 6 . when the pile driver ( not shown ) strikes plate 1 , the energy of the strike a is transmitted to plate 2 , which in turns drives casing 6 into the ground . accordingly , casing 6 bears the majority of the stresses in driving apparatus 100 into the ground . compound collar assembly 3 provides the means in which device 100 is to be lifted or extracted from the ground . referring to the fig2 and 3 , collar assembly 3 comprises an external lifting collar 14 , which encloses internal collar 4 . external casing 6 fits slidably within and through collar 4 . although the drawings shows collar 4 as a bolted on structure , collar 4 may also be a structure of a uniform ring . collar 14 has an annular space 27 ( see fig2 & amp ; 3 ) in which interior collar 4 fits within . lifting collar 14 is lifted by known lifting means such as cables 13 , which may be attached to a crane ( not shown ). as collar 14 is lifted by cables 13 , collar 14 engages interior collar 4 at 56 , which lifts mandrel 5 . internal collar 4 is affixed proximate to an upper portion of mandrel by pin 22 . however , collar 4 may be affixed to mandrel 5 by any number of methods , such as screws or bolts and the like , welding , or mandrel 5 and collar 4 can be casted or machined as a single item . in the preferred embodiment , pin 22 extends through a steel restraining member 40 . member 40 is restrained and securely affixed within the upper portion of mandrel 5 . member 40 may be made of any substantial and study material , e . g ., a solid steel plug , steel plating , or steel hollow metal cylinder . the ends of pin 22 extends out from opposing sides of the exterior of mandrel 5 and are fixed in a position located proximate to the upper portion of mandrel 5 . the ends of pin 22 interlocked with collar 4 , accordingly , so as pin 22 rides along and is restrained within slot 21 of casing 6 so does collar 4 . slot 21 is a located proximate the upper portion of casing 6 . therefore , as collar 4 moves up and down slot 21 so does mandrel 5 in relation to casing 6 . this can be seen when comparing fig2 and 3 . therefore , the interaction between pin 22 , collars 4 and 14 , and the bottom ledge of plate 2 , cause mandrel 5 to be displaced in relation to casing 6 when upward force b is applied . during the driving process , pile driving hammers offer cause pile mandrels to spiral which in turn would cause lifting cables to wind and tangle . this is avoided in collar assembly 3 . in fig2 , cables 13 exert no lifting force and therefore , interior collar 4 floats within annular space 27 , thus casing 6 may twist and turn with relative freedom within annular void 27 , without affecting cables 13 . suppressor 20 is located directly above member 40 . suppressor 20 is made from material possessing an ability to dampen vibrations , such materials can either be a natural material such as rubber or a synthetic elastomer . suppressor 20 as the drawings show is sandwiched between plate 1 and member 40 . the utility of suppressor 20 is discussed below . at the bottom of apparatus 100 are sealing means to control the release of grout or other pile forming materials . in the preferred embodiment , the sealing means comprise a pair of mating frusto conical profiles which is discussed as follows . at the bottom end of casing 6 is ground contacting member 28 , which comprises an open top end 32 and an open bottom end 31 . see fig2 and 3 . open top 32 end is located at the bottom end of casing 6 , and has an annular cross - sectional area with a downward facing frusto - conical profile . open top end 32 has a larger cross - sectional area than open bottom end 31 . ground contacting member 28 may be attached to the bottom of casing 6 either by a variety of means , such as screws , bolts , or by welding . attached to the bottom end of mandrel 5 is member 11 which has a substantially downward facing frusto - conical profile that is adapted to mate with ground contacting member &# 39 ; s ( 28 ) annular downward facing frusto - conical profile when the profiles come into contact with each other . member 11 also comprises a plug 8 which seals open bottom end 31 , an upward facing conical member 29 , and a plurality of elongated openings or slots 10 located radially along the upper portion of member 8 . the upper portion of member 11 is hollow , thereby allowing pile forming materials to exit openings 10 and migrate from the interior of mandrel 5 into the surrounding earth when the profiles are disengaged for each other . conical member 29 facilitates the dispersal of the pile forming materials out through openings 10 . located proximate to the bottom portion of casing 6 are a plurality of apertures 23 . during the driving of apparatus 100 into the ground , apertures 23 are in flow - through alignment with apertures 9 , which are located proximate the lower portion of mandrel 5 . this flow - through alignment of apertures 9 and 23 , depending on the type of pile being formed , enable cementitious fluid , water , or compressed air to exit from the interior of mandrel 5 and into the surrounding earth . in the case of installing a stone column air may be forced through these openings . this is discussed in further detail below . when it is time to form the pile , cables 13 exert an upward force b . mandrel 5 is lifted prior to casing 6 being engaged and lifted . see fig3 . this out of phase lifting of casing 6 and mandrel 5 results in lower plug 8 being lifted and unsealing the open bottom end 31 , which in turn allows the contents within mandrel 5 to be released through elongated slots 10 , out open bottom end 31 and into the hole created by apparatus 100 . as force b continues to lift mandrel 5 , collar 4 will ultimately engage the bottom of plate 2 at 55 , and thereby lift both casing 6 and mandrel 5 . during the entire lifting period , the hole is being filled with the contents of mandrel 5 . although not shown in the drawings , apparatus 100 may incorporate the use of one or more “ o ” rings 9 a to ensure an effective seal against unwanted grout , air , or water seepage when apertures 9 and 23 are out of phase . the “ o ” rings 9 a would be mounted on lower portion of mandrel 5 to a position above aperture 9 sealing the annular space between the exterior of mandrel 5 and the interior of casing 6 . the “ o ” rings 9 a may be made of any suitable elastomeric material that would be commonly used for the purpose of creating a seal , much like the piston rings of an automotive gasoline engine . apparatus 100 may also be equipped with an optional foot assembly 7 . such an assembly would be used when installing a pile formed from grout . as shown in the figures , foot assembly 7 is attached at the bottom of the exterior of casing 6 and encloses member 28 . foot assembly 7 includes a plate 50 , which is made of a sturdy material such as steel for contacting and penetrating the ground , cylindrical sidewall 72 which extends up over the lower portion of the member 28 , a plurality of removable bolts or screws 26 that are positioned radially proximate the lower portion of foot 7 , and sealing ring 75 with a sealing ring pin 24 which ride along in slot 80 . one end of bolts 26 rides along and is retrained within a channel 25 . channel 25 is located on the circumferential surface of member 28 . the boot is restrained to member 28 when bolts 26 are screwed in and ride up and down channel 25 . if foot assembly 7 is not desired , bolts 26 are removed and foot assembly 7 may slide off member 28 . sealing ring 75 comprises an annular ring of a sturdy material such as steel that circumferentially surrounds the a lower portion of the exterior casing 6 . sealing ring 75 is adapted so that it slideably fits around exterior casing 6 . during the driving process , sealing ring 75 is situated lower than the top edge of cylindrical sidewall 72 and below aligned apertures 9 and 23 . as shown in fig2 , so as not to hinder the flow of grout , air or water as the case may be , to enter the surrounding earth . when foot assembly is not used , ring 75 may be removed by removing pin 24 and sliding ring 75 off casing 6 ; ground contacting member 28 would also have to be removed . sealing ring 75 is activated via pin 24 . pin 24 is rigidly located proximate to the lower end of mandrel 5 , so as mandrel 5 traverse up and down within casing 6 , so does pin 24 . the distal ends of pin 24 extend through casing 6 and are interlocked into ring 75 , therefore , as pin 24 travels so does ring 75 . the travel of pin 24 , however , is confined within the boundaries of lower slot 80 . lower slot 80 is located on the lower portion of casing 6 . as indicated in the figures , slot 80 is aligned parallel to the length of both casing 6 and mandrel 5 . accordingly , when force b causes exterior collar 14 to engages interior collar 4 at point 56 , thereby causing mandrel 5 to be upwardly displaced in relation to casing 6 , pin 24 being interlocked to ring 75 causes ring 75 to also be displaced from its first position as seen in fig4 to a second position , as seen in fig5 , whereby , ring 75 seals the annular space 85 between the exterior of casing 6 and the interior of sidewall 72 . this sealing of annular space 85 aids in the extraction of device 100 from the earth by preventing debris from the surrounding earth from entering annular space 85 and clogging the extraction process or hanging up the device . another novel aspect of foot assembly 7 is that it provides a plume forming chamber 30 to facilitate the effective dispersal of grout into the surrounding earth . as shown in fig3 , 4 , and 5 during the extraction process mandrel 5 is displaced first in relation to casing 6 , when the upper portion of collar 14 engages the bottom of plate 2 at 55 , casing 6 begins to lift and is displaced in relation to the surrounding earth , accordingly , ground contacting member 28 lifts as casing 6 lifts , thereby causing the formation of chamber 30 . grout enters chamber 30 forming a steady state reservoir of grout , i . e ., the level within the reservoir will remain steady provided there is sufficient grout be delivered via mandrel 5 as the grout exits out opening 33 of chamber 30 . opening 33 as the figures show is larger in cross - sectional area than that of opening 31 . as apparatus 100 continues to lift from the ground , ground contacting member 28 engages foot assembly 7 when lifting bolt 26 contacts the bottom of channel 25 . at that point grout flows out of the grout reservoir formed in chamber 30 and into the surrounding earth . the affects of the chamber and the resultant reservoir causes the grout to flow more uniformly and widely into the earth . another novel aspect of foot assembly 7 , is that the foot is not left in the ground , but retrieved for subsequent and multiple use . this is advantageous as steel is not wasted and thus results in cost savings . the use of optional foot assembly 7 may be used where the desired hole dimensions is larger than can be created with casing 6 . accordingly , optional foot assembly 7 facilitates the flow , distribution , and placement of the pile forming material and can be adapted to the particular characteristics of pile forming material that is to be placed into the earth . in use , apparatus 100 is driven into the earth using known pile driving techniques and equipment . during the driving process , apertures 9 and 23 are in alignment , thereby allowing grout to flow from the interior of mandrel 5 through the apertures and into the surrounding earth . this provides fluid grout to lubricate the result pile hole to facilitate extraction of the apparatus 100 fills any voids with grout . pile driving force a maintains the plates 1 and 2 flush against each other and maintains the seal caused by the mating between the annular conical profile of ground contacting member 28 with the conical profile of mandrel end member 11 . when the desired depth is obtained , collar assembly 3 is use to lift the apparatus and extract apparatus 100 . see fig3 . during the extraction process , casing 6 and mandrel 5 are lifted out of phase , in that , mandrel 5 is first lifted and advances a certain distance prior to casing 6 being also lifted . when casing 6 begins lifting , both tubes are then lifted in unison . as mandrel 5 is displaced in an upwards direction from casing 6 , member 11 unseats from ground contacting member 28 , thereby creating an annular space . see fig3 . this causes grout to flow out of elongated slots 10 , out the bottom opening 31 , and into the surrounding earth . apparatus 100 may also be use to place stone column into the ground . when installing a stone pile optional foot 7 assembly would be used . instead of a grout delivery system as illustrated by 15 , apparatus 100 is equipped with a hopper ( not shown ), which supplies stone or crushed rock . mandrel 5 is not prefilled with stone , rocks and the like . compressed air or water is supplied to the annular space between casing 6 and mandrel 5 . in the placement of stone piles / columns , air or water is used to stabilize the surrounding earth as the device is being driven into the ground . the compressed air or water that is forced into the annual space between exterior casing 6 and mandrel 5 exits casing 6 through apertures 23 and into the surrounding earth . when the desired depth is reached , the hopper fills mandrel 5 with stone / rock , and the lifting procedure as described above begins . in addition , the lifting and depositing of stone may be halted and driving may restart so as to further compact the forming stone column . the water or compressed air that exits through aperture 23 assists in the extraction of the apparatus from the ground . when using apparatus 100 for installing stone or rock pile , optional foot 7 is not required . in addition apparatus 100 easily accommodates the application of vibration equipment . as as shown in the drawings , a suppressor 20 , which is made of an elastomeric material may be inserted within mandrel 5 . it is a common practice when placing stone columns to attached a vibrator to the mandrel . see , for example , the discussions in goughnour , u . s . pat . no . 5 , 279 , 502 . 1 in apparatus 100 , it is anticipated that a driving hammer will be used to drive the apparatus into the earth . accordingly , after driving , the hammer will rest on plate 1 . if a vibrator is used , suppressor 20 will substantially reduce the vibrations being transmitted to the hammer . excessive vibrations to the hammer may cause damage to the pile driving equipment . accordingly , no additional time or procedures will be required to remove the hammer from driving plate 1 . the hammer can ride on top of driving plate 1 during the entire lifting and vibrating process . 1 u . s . pat . no . 4 , 397 , 588 , col . 1 . ( regarding vibroflot ) a preferred embodiment of the invention has been described and illustrated for purposes of clarity and example , it must be understood that many changes , substitutions and modifications will become apparent to those possessed of ordinary skill in the art without thereby departing from the scope and spirit of the present invention which is defined by the following claims .'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['content'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "waiting-heaven",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the present invention is an apparatus and a method for casting a cementitious or stone pile into the ground . the present invention comprises an exterior driving casing and an interior mandrel acting in cooperation to hold and deliver the pile forming material . a retrievable driving driving shoe is also disclose .'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['title'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "marked-education",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34443"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "destroyed-dubai",
   "metadata": {},
   "outputs": [],
   "source": [
    "contractions =          {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\",\n",
    "                           \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n",
    "                           \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",\n",
    "                           \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\",\n",
    "                           \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n",
    "                           \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\",\n",
    "                           \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\",\n",
    "                           \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\n",
    "                           \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\",\n",
    "                           \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\",\n",
    "                           \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\",\n",
    "                           \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\",\n",
    "                           \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\",\n",
    "                           \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\",\n",
    "                           \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\",\n",
    "                           \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",\n",
    "                           \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\",\n",
    "                           \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\",\n",
    "                           \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\",\n",
    "                           \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\",\n",
    "                           \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
    "                           \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\",\n",
    "                           \"you're\": \"you are\", \"you've\": \"you have\", \"i've\": \"i have\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "quick-entry",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#for cleaning text \n",
    "def clean_text(text, remove_stopwords = True):\n",
    "    '''Remove unwanted characters, stopwords, and format the text to create fewer nulls word embeddings'''\n",
    "    \n",
    "    # Convert words to lower case\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Replace contractions with their longer forms \n",
    "    if True:\n",
    "        text = text.split()\n",
    "        new_text = []\n",
    "        for word in text:\n",
    "            if word in contractions:\n",
    "                new_text.append(contractions[word])\n",
    "            else:\n",
    "                new_text.append(word)\n",
    "        text = \" \".join(new_text)\n",
    "    \n",
    "    # Format words and remove unwanted characters\n",
    "    ##text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'\\<a href', ' ', text)\n",
    "    text = re.sub(r'&amp;', '', text) \n",
    "    ##text = re.sub(r'[_\"\\-;%()|+&=*%.,!?:#$@\\[\\]/]', ' ', text)\n",
    "    ##text = re.sub(r'<br />', ' ', text)\n",
    "    ##text = re.sub(r'\\'', ' ', text)\n",
    "    \n",
    "    # Optionally, remove stop words\n",
    "    if remove_stopwords:\n",
    "        text = text.split()\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        text = [w for w in text if not w in stops]\n",
    "        text = \" \".join(text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "parental-sperm",
   "metadata": {},
   "outputs": [],
   "source": [
    "dm_single_close_quote = u'\\u2019' # unicode\n",
    "dm_double_close_quote = u'\\u201d'\n",
    "END_TOKENS = ['.', '!', '?', '...', \"'\", \"`\", '\"', dm_single_close_quote, dm_double_close_quote, \")\"] # acceptable ways to end a sentence\n",
    "\n",
    "# We use these to separate the summary sentences in the .bin datafiles\n",
    "SENTENCE_START = '<s>'\n",
    "SENTENCE_END = '</s>'\n",
    "\n",
    "all_train_urls = \"\"\n",
    "all_val_urls = \"\"\n",
    "all_test_urls = \"\"\n",
    "\n",
    "cnn_tokenized_stories_dir_train = \"G:\\\\master thesis\\\\bigPatent_dataset_csv\\\\train\\\\cnn_stories_tokenized_train\" #location of folder to tokenize text\n",
    "cnn_tokenized_stories_dir_test = \"G:\\\\master thesis\\\\bigPatent_dataset_csv\\\\train\\\\cnn_stories_tokenized_test\" #location of folder to tokenize text\n",
    "cnn_tokenized_stories_dir_val = \"G:\\\\master thesis\\\\bigPatent_dataset_csv\\\\train\\\\cnn_stories_tokenized_val\" #location of folder to tokenize text\n",
    "dm_tokenized_stories_dir = \"G:\\\\master thesis\\\\bigPatent_dataset_csv\\\\train\\\\dm_stories_tokenized\" #not used\n",
    "finished_files_dir = \"G:\\\\master thesis\\\\bigPatent_dataset_csv\\\\train\\\\bp_finished_files\" #final ouput\n",
    "chunks_dir = os.path.join(finished_files_dir, \"chunked\")\n",
    "\n",
    "\n",
    "\n",
    "VOCAB_SIZE = 200000\n",
    "CHUNK_SIZE = 1000 # num examples per chunk, for the chunked data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "recovered-writing",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_file(set_name):\n",
    "    \n",
    "      in_file = finished_files_dir + '/%s.bin' % set_name\n",
    "      reader = open(in_file, \"rb\")\n",
    "      chunk = 0\n",
    "      finished = False\n",
    "      while not finished:\n",
    "        chunk_fname = os.path.join(chunks_dir, '%s_%03d.bin' % (set_name, chunk)) # new chunk\n",
    "        with open(chunk_fname, 'wb') as writer:\n",
    "          for _ in range(CHUNK_SIZE):\n",
    "            len_bytes = reader.read(8)\n",
    "            if not len_bytes:\n",
    "              finished = True\n",
    "              break\n",
    "            str_len = struct.unpack('q', len_bytes)[0]\n",
    "            example_str = struct.unpack('%ds' % str_len, reader.read(str_len))[0]\n",
    "            writer.write(struct.pack('q', str_len))\n",
    "            writer.write(struct.pack('%ds' % str_len, example_str))\n",
    "          chunk += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "arbitrary-friendly",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_all():\n",
    "  # Make a dir to hold the chunks\n",
    "  if not os.path.isdir(chunks_dir):\n",
    "    os.mkdir(chunks_dir)\n",
    "  # Chunk the data\n",
    "  for set_name in ['train', 'val', 'test']:\n",
    "    print (\"Splitting %s data into chunks...\" % set_name)\n",
    "    chunk_file(set_name)\n",
    "  print (\"Saved chunked data in %s\" % chunks_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "spatial-buffalo",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_stories(reviews, tokenized_stories_dir):\n",
    "  \"\"\"Maps a whole directory of .story files to a tokenized version using Stanford CoreNLP Tokenizer\"\"\"\n",
    "  #progress = ProgressBar.ProgressBar(len(reviews), fmt=ProgressBar.ProgressBar.FULL)\n",
    "  print('open progress bar')\n",
    "\n",
    "  for i, row in reviews.iterrows():\n",
    "        #if i==20:\n",
    "        #    break\n",
    "        filename = str(i) + '.tok'\n",
    "        with open(os.path.join(tokenized_stories_dir, filename), 'w', encoding=\"utf-8\") as temp_file:\n",
    "            text = row[\"content\"]\n",
    "            text = clean_text(text , remove_stopwords = True)\n",
    "            #tok = nltk.word_tokenize(text) #default\n",
    "            tok = nltk.sent_tokenize(text) #abdullah modified\n",
    "            tok.append(\"@highlight\")\n",
    "            Summary = row[\"title\"]\n",
    "            Summary = clean_text(Summary ,remove_stopwords = False)\n",
    "            #tok.extend(nltk.word_tokenize(Summary)) #default\n",
    "            tok.extend(nltk.sent_tokenize(Summary)) # abdullah modified\n",
    "            list = tok.copy()\n",
    "\n",
    "            for i in tok:\n",
    "                if(i=='``' or i==\"''\" ):\n",
    "                    list.remove(i)\n",
    "            tok_string = \"\\n\".join(str(x) for x in list)\n",
    "            temp_file.write(tok_string)\n",
    "\n",
    "        #progress.current += 1\n",
    "        #progress()\n",
    "  print (\"Successfully finished tokenizing to %s .\\n\" % (tokenized_stories_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "provincial-burton",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_missing_period(line):\n",
    "  \"\"\"Adds a period to a line that is missing a period\"\"\"\n",
    "  if \"@highlight\" in line: return line\n",
    "  if line==\"\": return line\n",
    "  if line[-1] in END_TOKENS: return line\n",
    "  # print line[-1]\n",
    "  return line + \" .\" # abdullah mofidied # default return line + \" .\"\n",
    "\n",
    "\n",
    "def read_text_file(text_file):\n",
    "  lines = []\n",
    "  with open(text_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "      lines.append(line.strip())\n",
    "  return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "embedded-headset",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_art_abs(story_file):\n",
    "  lines = read_text_file(story_file)\n",
    "\n",
    "  # Lowercase everything\n",
    "  lines = [line.lower() for line in lines]\n",
    "\n",
    "  # Put periods on the ends of lines that are missing them (this is a problem in the dataset because many image captions don't end in periods; consequently they end up in the body of the article as run-on sentences)\n",
    "  lines = [fix_missing_period(line) for line in lines]\n",
    "\n",
    "  # Separate out article and abstract sentences\n",
    "  article_lines = []\n",
    "  highlights = []\n",
    "  next_is_highlight = False\n",
    "  for idx,line in enumerate(lines):\n",
    "    if line == \"\":\n",
    "      continue # empty line\n",
    "    elif line.startswith(\"@highlight\"):\n",
    "      next_is_highlight = True \n",
    "    elif next_is_highlight: \n",
    "      highlights.append(line)\n",
    "    else:\n",
    "      article_lines.append(line)\n",
    "\n",
    "  # Make article into a single string\n",
    "  article = ' '.join(article_lines)\n",
    "\n",
    "  # Make abstract into a signle string, putting <s> and </s> tags around the sentences\n",
    "  abstract = ' '.join([\"%s %s %s\" % (SENTENCE_START, sent, SENTENCE_END) for sent in highlights])\n",
    "\n",
    "  return article, abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "british-vinyl",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def write_to_bin_train(file_names, out_file, makevocab=False):\n",
    "  \"\"\"Reads the tokenized .story files corresponding to the urls listed in the url_file and writes them to a out_file.\"\"\"\n",
    " \n",
    "  story_fnames = [str(s)+\".tok\" for s in file_names]\n",
    "  num_stories = len(story_fnames)\n",
    "\n",
    "  if makevocab:\n",
    "    vocab_counter = collections.Counter()\n",
    "\n",
    "  with open(out_file, 'wb') as writer:\n",
    "    for idx,s in enumerate(story_fnames):\n",
    "      if idx % 1000 == 0:\n",
    "        print( \"Writing story %i of %i; %.2f percent done\" % (idx, num_stories, float(idx)*100.0/float(num_stories)))\n",
    "\n",
    "      # Look in the tokenized story dirs to find the .story file corresponding to this url\n",
    "      if os.path.isfile(os.path.join(cnn_tokenized_stories_dir_train, s)):\n",
    "        story_file = os.path.join(cnn_tokenized_stories_dir_train, s)\n",
    "      elif os.path.isfile(os.path.join(dm_tokenized_stories_dir, s)):\n",
    "        story_file = os.path.join(dm_tokenized_stories_dir, s)\n",
    "      else:\n",
    "        print (\"Error: Couldn't find tokenized story file %s in either tokenized story directories %s and %s. Was there an error during tokenization?\" % (s, cnn_tokenized_stories_dir, dm_tokenized_stories_dir))\n",
    "        # Check again if tokenized stories directories contain correct number of files\n",
    "        print (\"Checking that the tokenized stories directories %s and %s contain correct number of files...\" % (cnn_tokenized_stories_dir, dm_tokenized_stories_dir))\n",
    "        #check_num_stories(cnn_tokenized_stories_dir, num_expected_cnn_stories)\n",
    "        #check_num_stories(dm_tokenized_stories_dir, num_expected_dm_stories)\n",
    "        #raise Exception(\"Tokenized stories directories %s and %s contain correct number of files but story file %s found in neither.\" % (cnn_tokenized_stories_dir, dm_tokenized_stories_dir, s))\n",
    "        \n",
    "      # Get the strings to write to .bin file\n",
    "      article, abstract = get_art_abs(story_file)\n",
    "\n",
    "      \n",
    "      # Write to tf.Example\n",
    "      tf_example = example_pb2.Example()\n",
    "      tf_example.features.feature['article'].bytes_list.value.extend([article.encode('utf-8')])\n",
    "      tf_example.features.feature['abstract'].bytes_list.value.extend([abstract.encode('utf-8')])\n",
    "      tf_example_str = tf_example.SerializeToString()\n",
    "      str_len = len(tf_example_str)\n",
    "      writer.write(struct.pack('q', str_len))\n",
    "      writer.write(struct.pack('%ds' % str_len, tf_example_str))\n",
    "   \n",
    "\n",
    "      # Write the vocab to file, if applicable\n",
    "      if makevocab:\n",
    "        art_tokens = article.split(' ')\n",
    "        abs_tokens = abstract.split(' ')\n",
    "        abs_tokens = [t for t in abs_tokens if t not in [SENTENCE_START, SENTENCE_END]] # remove these tags from vocab\n",
    "        tokens = art_tokens + abs_tokens\n",
    "        tokens = [t.strip() for t in tokens] # strip\n",
    "        tokens = [t for t in tokens if t!=\"\"] # remove empty\n",
    "        vocab_counter.update(tokens)\n",
    "\n",
    "  print (\"Finished writing file %s\\n\" % out_file)\n",
    "\n",
    "  # write vocab to file\n",
    "  if makevocab:\n",
    "    print (\"Writing vocab file...\")\n",
    "    with open(os.path.join(finished_files_dir, \"vocab\"), 'w', encoding=\"utf-8\") as writer:\n",
    "      for word, count in vocab_counter.most_common(VOCAB_SIZE):\n",
    "        writer.write(word + ' ' + str(count) + '\\n')\n",
    "    print (\"Finished writing vocab file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "imported-indianapolis",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def write_to_bin_test(file_names, out_file, makevocab=False):\n",
    "  \"\"\"Reads the tokenized .story files corresponding to the urls listed in the url_file and writes them to a out_file.\"\"\"\n",
    " \n",
    "  story_fnames = [str(s)+\".tok\" for s in file_names]\n",
    "  num_stories = len(story_fnames)\n",
    "\n",
    "  if makevocab:\n",
    "    vocab_counter = collections.Counter()\n",
    "\n",
    "  with open(out_file, 'wb') as writer:\n",
    "    for idx,s in enumerate(story_fnames):\n",
    "      if idx % 1000 == 0:\n",
    "        print( \"Writing story %i of %i; %.2f percent done\" % (idx, num_stories, float(idx)*100.0/float(num_stories)))\n",
    "\n",
    "      # Look in the tokenized story dirs to find the .story file corresponding to this url\n",
    "      if os.path.isfile(os.path.join(cnn_tokenized_stories_dir_test, s)):\n",
    "        story_file = os.path.join(cnn_tokenized_stories_dir_test, s)\n",
    "      elif os.path.isfile(os.path.join(dm_tokenized_stories_dir, s)):\n",
    "        story_file = os.path.join(dm_tokenized_stories_dir, s)\n",
    "      else:\n",
    "        print (\"Error: Couldn't find tokenized story file %s in either tokenized story directories %s and %s. Was there an error during tokenization?\" % (s, cnn_tokenized_stories_dir, dm_tokenized_stories_dir))\n",
    "        # Check again if tokenized stories directories contain correct number of files\n",
    "        print (\"Checking that the tokenized stories directories %s and %s contain correct number of files...\" % (cnn_tokenized_stories_dir, dm_tokenized_stories_dir))\n",
    "        #check_num_stories(cnn_tokenized_stories_dir, num_expected_cnn_stories)\n",
    "        #check_num_stories(dm_tokenized_stories_dir, num_expected_dm_stories)\n",
    "        #raise Exception(\"Tokenized stories directories %s and %s contain correct number of files but story file %s found in neither.\" % (cnn_tokenized_stories_dir, dm_tokenized_stories_dir, s))\n",
    "        \n",
    "      # Get the strings to write to .bin file\n",
    "      article, abstract = get_art_abs(story_file)\n",
    "\n",
    "      \n",
    "      # Write to tf.Example\n",
    "      tf_example = example_pb2.Example()\n",
    "      tf_example.features.feature['article'].bytes_list.value.extend([article.encode('utf-8')])\n",
    "      tf_example.features.feature['abstract'].bytes_list.value.extend([abstract.encode('utf-8')])\n",
    "      tf_example_str = tf_example.SerializeToString()\n",
    "      str_len = len(tf_example_str)\n",
    "      writer.write(struct.pack('q', str_len))\n",
    "      writer.write(struct.pack('%ds' % str_len, tf_example_str))\n",
    "   \n",
    "\n",
    "      # Write the vocab to file, if applicable\n",
    "      if makevocab:\n",
    "        art_tokens = article.split(' ')\n",
    "        abs_tokens = abstract.split(' ')\n",
    "        abs_tokens = [t for t in abs_tokens if t not in [SENTENCE_START, SENTENCE_END]] # remove these tags from vocab\n",
    "        tokens = art_tokens + abs_tokens\n",
    "        tokens = [t.strip() for t in tokens] # strip\n",
    "        tokens = [t for t in tokens if t!=\"\"] # remove empty\n",
    "        vocab_counter.update(tokens)\n",
    "\n",
    "  print (\"Finished writing file %s\\n\" % out_file)\n",
    "\n",
    "  # write vocab to file\n",
    "  if makevocab:\n",
    "    print (\"Writing vocab file...\")\n",
    "    with open(os.path.join(finished_files_dir, \"vocab\"), 'w', encoding=\"utf-8\") as writer:\n",
    "      for word, count in vocab_counter.most_common(VOCAB_SIZE):\n",
    "        writer.write(word + ' ' + str(count) + '\\n')\n",
    "    print (\"Finished writing vocab file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "pending-updating",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def write_to_bin_val(file_names, out_file, makevocab=False):\n",
    "  \"\"\"Reads the tokenized .story files corresponding to the urls listed in the url_file and writes them to a out_file.\"\"\"\n",
    " \n",
    "  story_fnames = [str(s)+\".tok\" for s in file_names]\n",
    "  num_stories = len(story_fnames)\n",
    "\n",
    "  if makevocab:\n",
    "    vocab_counter = collections.Counter()\n",
    "\n",
    "  with open(out_file, 'wb') as writer:\n",
    "    for idx,s in enumerate(story_fnames):\n",
    "      if idx % 1000 == 0:\n",
    "        print( \"Writing story %i of %i; %.2f percent done\" % (idx, num_stories, float(idx)*100.0/float(num_stories)))\n",
    "\n",
    "      # Look in the tokenized story dirs to find the .story file corresponding to this url\n",
    "      if os.path.isfile(os.path.join(cnn_tokenized_stories_dir_val, s)):\n",
    "        story_file = os.path.join(cnn_tokenized_stories_dir_val, s)\n",
    "      elif os.path.isfile(os.path.join(dm_tokenized_stories_dir, s)):\n",
    "        story_file = os.path.join(dm_tokenized_stories_dir, s)\n",
    "      else:\n",
    "        print (\"Error: Couldn't find tokenized story file %s in either tokenized story directories %s and %s. Was there an error during tokenization?\" % (s, cnn_tokenized_stories_dir, dm_tokenized_stories_dir))\n",
    "        # Check again if tokenized stories directories contain correct number of files\n",
    "        print (\"Checking that the tokenized stories directories %s and %s contain correct number of files...\" % (cnn_tokenized_stories_dir, dm_tokenized_stories_dir))\n",
    "        #check_num_stories(cnn_tokenized_stories_dir, num_expected_cnn_stories)\n",
    "        #check_num_stories(dm_tokenized_stories_dir, num_expected_dm_stories)\n",
    "        #raise Exception(\"Tokenized stories directories %s and %s contain correct number of files but story file %s found in neither.\" % (cnn_tokenized_stories_dir, dm_tokenized_stories_dir, s))\n",
    "        \n",
    "      # Get the strings to write to .bin file\n",
    "      article, abstract = get_art_abs(story_file)\n",
    "\n",
    "      \n",
    "      # Write to tf.Example\n",
    "      tf_example = example_pb2.Example()\n",
    "      tf_example.features.feature['article'].bytes_list.value.extend([article.encode('utf-8')])\n",
    "      tf_example.features.feature['abstract'].bytes_list.value.extend([abstract.encode('utf-8')])\n",
    "      tf_example_str = tf_example.SerializeToString()\n",
    "      str_len = len(tf_example_str)\n",
    "      writer.write(struct.pack('q', str_len))\n",
    "      writer.write(struct.pack('%ds' % str_len, tf_example_str))\n",
    "   \n",
    "\n",
    "      # Write the vocab to file, if applicable\n",
    "      if makevocab:\n",
    "        art_tokens = article.split(' ')\n",
    "        abs_tokens = abstract.split(' ')\n",
    "        abs_tokens = [t for t in abs_tokens if t not in [SENTENCE_START, SENTENCE_END]] # remove these tags from vocab\n",
    "        tokens = art_tokens + abs_tokens\n",
    "        tokens = [t.strip() for t in tokens] # strip\n",
    "        tokens = [t for t in tokens if t!=\"\"] # remove empty\n",
    "        vocab_counter.update(tokens)\n",
    "\n",
    "  print (\"Finished writing file %s\\n\" % out_file)\n",
    "\n",
    "  # write vocab to file\n",
    "  if makevocab:\n",
    "    print (\"Writing vocab file...\")\n",
    "    with open(os.path.join(finished_files_dir, \"vocab\"), 'w', encoding=\"utf-8\") as writer:\n",
    "      for word, count in vocab_counter.most_common(VOCAB_SIZE):\n",
    "        writer.write(word + ' ' + str(count) + '\\n')\n",
    "    print (\"Finished writing vocab file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "wrong-thread",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_num_stories(stories_dir, num_expected):\n",
    "  num_stories = len(os.listdir(stories_dir))\n",
    "  if num_stories != num_expected:\n",
    "    raise Exception(\"stories directory %s contains %i files but should contain %i\" % (stories_dir, num_stories, num_expected))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "loaded-tunnel",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "open progress bar\n",
      "Successfully finished tokenizing to G:\\master thesis\\bigPatent_dataset_csv\\train\\cnn_stories_tokenized_train .\n",
      "\n",
      "open progress bar\n",
      "Successfully finished tokenizing to G:\\master thesis\\bigPatent_dataset_csv\\train\\cnn_stories_tokenized_test .\n",
      "\n",
      "open progress bar\n",
      "Successfully finished tokenizing to G:\\master thesis\\bigPatent_dataset_csv\\train\\cnn_stories_tokenized_val .\n",
      "\n",
      "Writing story 0 of 1914; 0.00 percent done\n",
      "Writing story 1000 of 1914; 52.25 percent done\n",
      "Finished writing file G:\\master thesis\\bigPatent_dataset_csv\\train\\bp_finished_files\\test.bin\n",
      "\n",
      "Writing story 0 of 1914; 0.00 percent done\n",
      "Writing story 1000 of 1914; 52.25 percent done\n",
      "Finished writing file G:\\master thesis\\bigPatent_dataset_csv\\train\\bp_finished_files\\val.bin\n",
      "\n",
      "Writing story 0 of 34443; 0.00 percent done\n",
      "Writing story 1000 of 34443; 2.90 percent done\n",
      "Writing story 2000 of 34443; 5.81 percent done\n",
      "Writing story 3000 of 34443; 8.71 percent done\n",
      "Writing story 4000 of 34443; 11.61 percent done\n",
      "Writing story 5000 of 34443; 14.52 percent done\n",
      "Writing story 6000 of 34443; 17.42 percent done\n",
      "Writing story 7000 of 34443; 20.32 percent done\n",
      "Writing story 8000 of 34443; 23.23 percent done\n",
      "Writing story 9000 of 34443; 26.13 percent done\n",
      "Writing story 10000 of 34443; 29.03 percent done\n",
      "Writing story 11000 of 34443; 31.94 percent done\n",
      "Writing story 12000 of 34443; 34.84 percent done\n",
      "Writing story 13000 of 34443; 37.74 percent done\n",
      "Writing story 14000 of 34443; 40.65 percent done\n",
      "Writing story 15000 of 34443; 43.55 percent done\n",
      "Writing story 16000 of 34443; 46.45 percent done\n",
      "Writing story 17000 of 34443; 49.36 percent done\n",
      "Writing story 18000 of 34443; 52.26 percent done\n",
      "Writing story 19000 of 34443; 55.16 percent done\n",
      "Writing story 20000 of 34443; 58.07 percent done\n",
      "Writing story 21000 of 34443; 60.97 percent done\n",
      "Writing story 22000 of 34443; 63.87 percent done\n",
      "Writing story 23000 of 34443; 66.78 percent done\n",
      "Writing story 24000 of 34443; 69.68 percent done\n",
      "Writing story 25000 of 34443; 72.58 percent done\n",
      "Writing story 26000 of 34443; 75.49 percent done\n",
      "Writing story 27000 of 34443; 78.39 percent done\n",
      "Writing story 28000 of 34443; 81.29 percent done\n",
      "Writing story 29000 of 34443; 84.20 percent done\n",
      "Writing story 30000 of 34443; 87.10 percent done\n",
      "Writing story 31000 of 34443; 90.00 percent done\n",
      "Writing story 32000 of 34443; 92.91 percent done\n",
      "Writing story 33000 of 34443; 95.81 percent done\n",
      "Writing story 34000 of 34443; 98.71 percent done\n",
      "Finished writing file G:\\master thesis\\bigPatent_dataset_csv\\train\\bp_finished_files\\train.bin\n",
      "\n",
      "Writing vocab file...\n",
      "Finished writing vocab file\n",
      "Splitting train data into chunks...\n",
      "Splitting val data into chunks...\n",
      "Splitting test data into chunks...\n",
      "Saved chunked data in G:\\master thesis\\bigPatent_dataset_csv\\train\\bp_finished_files\\chunked\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == '__main__':\n",
    "  #main directory\n",
    "  cnn_stories_dir =  r\"G:\\\\master thesis\\\\bigPatent_dataset_csv\\\\train\"\n",
    "\n",
    "  ####### modified by abdullah (for training data) ################\n",
    "  # Create some new directories\n",
    "  if not os.path.exists(cnn_tokenized_stories_dir_train): os.makedirs(cnn_tokenized_stories_dir_train)\n",
    "  if not os.path.exists(finished_files_dir): os.makedirs(finished_files_dir)\n",
    "\n",
    "  #data needed is in a csv format\n",
    "  #containg 2 columbs (content , title)\n",
    "  reviews_csv =cnn_stories_dir + \"\\\\tr_pgn_E.csv\"\n",
    "  reviews_tr = pd.read_csv(reviews_csv)\n",
    "  reviews_tr = reviews_tr.filter(['content', 'title'])\n",
    "  reviews_tr = reviews_tr.dropna()\n",
    "  reviews_tr = reviews_tr.reset_index(drop=True)\n",
    "  reviews_tr.head()\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "  # Run nltk tokenizer on both text and summary , outputting to tokenized stories directories\n",
    "  tokenize_stories(reviews_tr, cnn_tokenized_stories_dir_train)\n",
    "\n",
    "  #to get the length of your dataset\n",
    "  num_expected_cnn_stories_tr =reviews_tr.shape[0]\n",
    "    \n",
    "    \n",
    "    \n",
    "   ####### modified by abdullah (for training data) ################\n",
    "  # Create some new directories\n",
    "  if not os.path.exists(cnn_tokenized_stories_dir_test): os.makedirs(cnn_tokenized_stories_dir_test)\n",
    "  #if not os.path.exists(finished_files_dir): os.makedirs(finished_files_dir)\n",
    "\n",
    "  #data needed is in a csv format\n",
    "  #containg 2 columbs (content , title)\n",
    "  reviews_csv =cnn_stories_dir + \"\\\\ts_pgn_E.csv\"\n",
    "  reviews_ts = pd.read_csv(reviews_csv)\n",
    "  reviews_ts = reviews_ts.filter(['content', 'title'])\n",
    "  reviews_ts = reviews_ts.dropna()\n",
    "  reviews_ts = reviews_ts.reset_index(drop=True)\n",
    "  reviews_ts.head()\n",
    "\n",
    "\n",
    "# Run nltk tokenizer on both text and summary , outputting to tokenized stories directories\n",
    "  tokenize_stories(reviews_ts, cnn_tokenized_stories_dir_test)\n",
    "\n",
    "  #to get the length of your dataset\n",
    "  num_expected_cnn_stories_ts =reviews_ts.shape[0]\n",
    "\n",
    "    \n",
    "    \n",
    "  ####### modified by abdullah (for training data) ################\n",
    "  # Create some new directories\n",
    "  if not os.path.exists(cnn_tokenized_stories_dir_val): os.makedirs(cnn_tokenized_stories_dir_val)\n",
    "  #if not os.path.exists(finished_files_dir): os.makedirs(finished_files_dir)\n",
    "\n",
    "  #data needed is in a csv format\n",
    "  #containg 2 columbs (content , title)\n",
    "  reviews_csv =cnn_stories_dir + \"\\\\val_pgn_E.csv\"\n",
    "  reviews_val = pd.read_csv(reviews_csv)\n",
    "  reviews_val = reviews_val.filter(['content', 'title'])\n",
    "  reviews_val = reviews_val.dropna()\n",
    "  reviews_val = reviews_val.reset_index(drop=True)\n",
    "  reviews_val.head()  \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "  # Run nltk tokenizer on both text and summary , outputting to tokenized stories directories\n",
    "  tokenize_stories(reviews_val, cnn_tokenized_stories_dir_val)\n",
    "\n",
    "  #to get the length of your dataset\n",
    "  num_expected_cnn_stories_val =reviews_val.shape[0]  \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "  #testing len = 2000\n",
    "  #validation lenght = 2000\n",
    "  ##all_train_urls = range(0,num_expected_cnn_stories-2000)\n",
    "  ##all_val_urls = range(num_expected_cnn_stories-2000,num_expected_cnn_stories-1000)\n",
    "  ##all_test_urls = range(num_expected_cnn_stories-1000,num_expected_cnn_stories)\n",
    "\n",
    "  all_train_urls = range(0,num_expected_cnn_stories_tr)\n",
    "  all_val_urls = range(0,num_expected_cnn_stories_val)\n",
    "  all_test_urls = range(0,num_expected_cnn_stories_ts)\n",
    "\n",
    "  #for testing\n",
    "  ##############all_train_urls= range(0,80)\n",
    "  ##############all_val_urls = range(80,90)\n",
    "  ##############all_test_urls = range(90,100)\n",
    "\n",
    "  # Read the tokenized stories, do a little postprocessing then write to bin files\n",
    "  write_to_bin_test(all_test_urls, os.path.join(finished_files_dir, \"test.bin\"))\n",
    "  write_to_bin_val(all_val_urls, os.path.join(finished_files_dir, \"val.bin\"))\n",
    "  write_to_bin_train(all_train_urls, os.path.join(finished_files_dir, \"train.bin\"), makevocab=True)\n",
    "\n",
    "  # Chunk the data. This splits each of train.bin, val.bin and test.bin into smaller chunks, each containing e.g. 1000 examples, and saves them in finished_files/chunks\n",
    "  chunk_all()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
